{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start PyTorch\n",
    "\n",
    "Welcome to FloydHub! If you're here, then you've just achieved your first success on FloydHub: running your first GPU-powered Job. ðŸŽ‰ Congrats! Now let's go through the classic 'Hello World' of deep learning: the hand-written digit classification task. In this example, we'll be using PyTorch as our deep learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First things first - how to set up for success\n",
    "\n",
    "Let's first outline which machine learning packages we'll need to run this experiment:\n",
    "\n",
    "- [Numpy](http://www.numpy.org/) is the fundamental package for scientific computing with Python\n",
    "- [PyTorch](http://pytorch.org/) is the the deep learning framework we'll be using this example\n",
    "- [torchvision](http://pytorch.org/docs/master/torchvision/index.html) package consists of popular datasets, model architectures, and common image transformations for computer vision\n",
    "- [matplotlib](https://matplotlib.org/) is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "\n",
    "Note: the most important thing to learn when run a Jupyter Notebook is the **`Shift + Enter`** shortcut, which runs any command in a Code Cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Seed for replicability\n",
    "torch.manual_seed(1)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## The MNIST Dataset\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset is available for free and provides a great starting point for experimenting with deep learning. Normally, we'd want to create a separate Dataset on FloydHub for our data - but, in this case, the torchvision package actually provides a direct MNIST API that we'll be using.\n",
    "\n",
    "## Experiment structure\n",
    "PyTorch experiments are typically structured in this way:\n",
    "\n",
    "* Create a PyTorch dataset (if not already provided by the framework)\n",
    "* Create an iterator on the dataset using the DataLoader API. This combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "Note below that we have specified the `/MNIST` folder as an absolute path - this is important. If we were to use the local directory, then we would have been mixing the output of the instance with our input data, which would consuming additional space and mixing code with data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "num_workers = 1\n",
    "\n",
    "# Download the MNIST dataset into the /MNIST folder if not mounted via FH --data\n",
    "train = MNIST('/MNIST', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "if cuda:\n",
    "    # Create DataLoader for cuda\n",
    "    dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "else:\n",
    "    dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=0, pin_memory=False)\n",
    "    \n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train.train_data\n",
    "train_data = train.transform(train_data.numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train.train_data.size())\n",
    "print(' - Transformed Shape:', train_data.size())\n",
    "print(' - min:', torch.min(train_data))\n",
    "print(' - max:', torch.max(train_data))\n",
    "print(' - mean:', torch.mean(train_data))\n",
    "print(' - std:', torch.std(train_data))\n",
    "print(' - var:', torch.var(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test.test_data\n",
    "test_data = test.transform(test_data.numpy())\n",
    "\n",
    "print('[Test]')\n",
    "print(' - Numpy Shape:', test.test_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', test.test_data.size())\n",
    "print(' - Transformed Shape:', test_data.size())\n",
    "print(' - min:', torch.min(test_data))\n",
    "print(' - max:', torch.max(test_data))\n",
    "print(' - mean:', torch.mean(test_data))\n",
    "print(' - std:', torch.std(test_data))\n",
    "print(' - var:', torch.var(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 60k black & white images of 28x28 pixels for training, and 10k for testing. Let's visualize what this samples look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show a sample\n",
    "plt.imshow(test.test_data.cpu()[0].numpy(), cmap='gray')\n",
    "plt.title('DIGIT: %i' % test.test_labels.cpu()[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is what it looks like seen as raw numbers in range 0-254,\n",
    "# where 0 represent black pixels and 254 white pixels.\n",
    "\n",
    "print (test.test_data.cpu()[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More Visualization on grid format\n",
    "\n",
    "# Plot a grid of images\n",
    "def imshow(inp_tensor, title):\n",
    "    \"\"\"image show for Tensor\"\"\"\n",
    "    img = inp_tensor.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "# Get a batch\n",
    "trainset_iter = iter(train_loader)\n",
    "images, labels = trainset_iter.next()\n",
    "# show images\n",
    "batch_imgs = make_grid(images, nrow=batch_size)\n",
    "plt.axis('off')\n",
    "imshow(batch_imgs, title=[x for x in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to prepare the DataLoader to perform the training - we have increased the batch size reducing the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "\n",
    "if cuda:\n",
    "    # Create DataLoader for cuda\n",
    "    dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "else:\n",
    "    dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=0, pin_memory=False)\n",
    "    \n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We use a 2 layer Neural Networks with ReLU as activation function, BatchNormalization and Dropout as regularizer. What does it means all these things:\n",
    "\n",
    "- [Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network), a class of statistical models which take inspiration from the Human Neural System and represent the computation as a DAG(Direct Acyclic Graph), for a great introduction see this video: [But what *is* a Neural Network? | Deep learning](https://youtu.be/aircAruvnKk).\n",
    "- [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is the activation function, a non-linearity operation which takes inspiration from human biology. It's the magic that drive the learning process.\n",
    "- [Batch Normalization](https://arxiv.org/abs/1502.03167). Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.\n",
    "- [Dropout](http://jmlr.org/papers/v15/srivastava14a.html) is a technique which randomly drop units (along with their connections) from the neural network during training. This prevent the overfitting(bad generalization) in Deep neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784 # 28 * 28\n",
    "hidden_size = 548\n",
    "hidden_size2 = 252\n",
    "num_classes = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bc1 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size2)\n",
    "        self.bc2 = nn.BatchNorm1d(hidden_size2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 784))\n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        out = F.log_softmax(h)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "\n",
    "if cuda:\n",
    "    model.cuda() # CUDA!\n",
    "\n",
    "# Cross Entropy as loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# If you are running a GPU instance, compute the loss on GPU\n",
    "if cuda:\n",
    "    loss_fn.cuda()    \n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Perform a 5 epochs(5 time over the full dataset) training and plot Loss and Accuracy over the epochs. Why do we make this? Training is driven by 2 metrics:\n",
    "\n",
    "- Minimize the Loss, means that the error between model prediction and datatset label must the less as possible. This metric drives the training since the model parameters are update with respect to the Loss to improve the prediction.\n",
    "- High Accuracy, means to correctly predict the sample to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "print_every = 100\n",
    "\n",
    "# Metrics\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "\n",
    "# Model train mode\n",
    "model.train()\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # image unrolling\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        if cuda:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Load loss on CPU\n",
    "        if cuda:\n",
    "            loss.cpu()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### Keep track of metric every batch\n",
    "        # Loss Metric\n",
    "        train_loss.append(loss.data[0])\n",
    "        # Accuracy Metric\n",
    "        prediction = outputs.data.max(1)[1]   # first column has actual prob.\n",
    "        accuracy = prediction.eq(labels.data).sum()/batch_size*100\n",
    "        train_accu.append(accuracy)\n",
    "        \n",
    "        # Log\n",
    "        if (i+1) % print_every == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f, Accuracy: %.4f' \n",
    "                   % (epoch+1, num_epochs, i+1, len(train)//batch_size, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Loss over time\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss function over time\")\n",
    "plt.plot(np.arange(len(train_loss)), train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Accuracy over time\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy function over time\")\n",
    "plt.plot(np.arange(len(train_accu)), train_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Evaluate the trained model over the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data.view(-1, 28*28), volatile=True), Variable(target)\n",
    "    if cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    output = model(data)\n",
    "    # Load output on CPU\n",
    "    if cuda:\n",
    "        output.cpu()\n",
    "    prediction = output.data.max(1)[1]\n",
    "    correct += prediction.eq(target.data).sum()\n",
    "\n",
    "print('\\nTest set: Accuracy: {:.2f}%'.format(100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Great! You achieved 98% accuracy and familiarized yourself with Jupyter Notebooks running on FloydHub. This is just the beginning of your deep learning journey on FloydHub! We're excited to see what your next experiments.\n",
    "\n",
    "If you need help with deep learning content or you cannot find an answer to your questions in our [documentation](https://docs.floydhub.com/), then you can open a topic in our community [forum](https://forum.floydhub.com/). We're eager to continue improving FloydHub to help your deep learning journey.\n",
    "\n",
    "Happy Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
